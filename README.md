# On-Device_LLMs_Gemma

## Implementation

To set up and run the MediaPipe LLM Inference task for web applications, follow these steps:

1. Ensure your browser supports WebGPU, like Chrome on macOS or Windows.
2. Create a folder named llm_task.
3. Copy index.html and index.js files into your llm_task folder.
4. Download the Gemma 2B model from Gemma or convert an external LLM model (Phi-2, Falcon, or StableLM) into the llm_task folder, ensuring itâ€™s compatible with a GPU backend.
5. In the index.js file, update the modelFileName variable to match your model fileâ€™s name.
6. Run a local server within the llm_task folder using the command python -m http.server 8080 or python -m SimpleHTTPServer 8080 for older Python versions.
7. Open localhost:8080 in your Chrome browser. The web interface will activate, ready for use in about 10 seconds.

---------------------------------------------------------------------------
# About MeğŸš€

Hello! Iâ€™m Toni Ramchandani ğŸ‘‹. Iâ€™m deeply passionate about all things technology! My journey is about exploring the vast and dynamic world of tech, from cutting-edge innovations to practical business solutions. I believe in the power of technology to transform our lives and work. ğŸŒ

Letâ€™s connect at https://www.linkedin.com/in/toni-ramchandani/ and exchange ideas about the latest tech trends and advancements! ğŸŒŸ
